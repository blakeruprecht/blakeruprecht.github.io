<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Researches on Blake Ruprecht</title>
    <link>https://blakeruprecht.github.io/research/</link>
    <description>Recent content in Researches on Blake Ruprecht</description>
    <generator>Hugo -- gohugo.io</generator><atom:link href="https://blakeruprecht.github.io/research/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Research</title>
      <link>https://blakeruprecht.github.io/research/research/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/research/research/</guid>
      <description>Previously, I was an Artificial Intelligence research assistant at the University of Missouri (MIZ-ZOU!) working with Derek Anderson and the Center for Geospatial Intelligence. Our work focused on eXplainable AI (XAI), specifically developing new neuro-fuzzy systems.
I researched the AI Alignment problem, which is in the same category as AI safety research, risk research, etc., alignment is the modern term used to describe developing artificial intelligences that have the same goals as humans and are trustworthy and safe to use.</description>
    </item>
    
  </channel>
</rss>
