<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>About me on Blake Ruprecht</title>
    <link>https://blakeruprecht.github.io/</link>
    <description>Recent content in About me on Blake Ruprecht</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Mon, 24 Jul 2023 15:29:23 -0400</lastBuildDate><atom:link href="https://blakeruprecht.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>My quest for the perfect bowl of Noodle Soup</title>
      <link>https://blakeruprecht.github.io/blog/noodle-soup/</link>
      <pubDate>Fri, 29 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/blog/noodle-soup/</guid>
      <description>Holy shit, it&amp;rsquo;s all coming together. After watching Kung Fu Panda as a kid, I&amp;rsquo;ve been craving a delicious Asian nooodle-soup dish. Instant ramen is a good solution for $1 and a heart attack, but I want something fresh, wholesome, and delicious. Before this journey, ramen never did it for me, and other dishes like Pad Thai and Lo Mein just didn&amp;rsquo;t hit the spot.
I never liked Thai food, and I&amp;rsquo;m a serious food explorer.</description>
    </item>
    
    <item>
      <title>Do it yourself EV charger installation</title>
      <link>https://blakeruprecht.github.io/blog/install-an-ev-charger/</link>
      <pubDate>Wed, 27 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/blog/install-an-ev-charger/</guid>
      <description>For the record, I wouldn&amp;rsquo;t recommend you install an EV charger yourself if you have no electrical experience. It&amp;rsquo;s extremely dangerous, and if you mess up, you can very easily catch your house on fire, or worse. However, it&amp;rsquo;s cool to at least now the process in theory, and this way, you can see if your electrician is doing it properly as well.
How much does it cost? If it was free, everyone would do it.</description>
    </item>
    
    <item>
      <title>Insects are a great source of nutrients</title>
      <link>https://blakeruprecht.github.io/blog/insect-farming/</link>
      <pubDate>Sat, 23 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/blog/insect-farming/</guid>
      <description>I ate a scorpion in Bangkok and it wasn&amp;rsquo;t good. The outer shell dominated the texture, and the meat tasted vaguely stale. Would not recommend.
I also ate grasshoppers, crickets, mealworms, and bamboo worms. I liked them all, and liked the mealworms so much I bought them many more times for a crunchy snack. They reminded me of a meatier cheese puff &amp;ndash; salty, crunchy, and airy. The crickets had much more substance to them, but were suprisingly good, with a roasted, fishy flavor.</description>
    </item>
    
    <item>
      <title>I learned Python through AI research</title>
      <link>https://blakeruprecht.github.io/blog/ai-to-learn-python/</link>
      <pubDate>Sun, 17 Mar 2024 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/blog/ai-to-learn-python/</guid>
      <description>I learned Python as an AI researcher, since Python is the industry standard for open-source AI software. Python isn&amp;rsquo;t particularly good at any one thing language wise, but it&amp;rsquo;s pretty much the jack-of-all-trades, can do anything language, so it&amp;rsquo;s great for prototyping algorithms that don&amp;rsquo;t make any sense. It took me forever to realize that most of what people were doing with Python wasn&amp;rsquo;t that complicated. Even though there are a million different packages to learn, each package basically does one thing, and does it well.</description>
    </item>
    
    <item>
      <title>Simple software follows Unix principles</title>
      <link>https://blakeruprecht.github.io/blog/simple-software/</link>
      <pubDate>Mon, 20 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/blog/simple-software/</guid>
      <description>Software is hard I just want to say that I find software quite difficult to work with, and I in theory have a masters degree in computer science. My argument against that is my degree focuses on math, not programming, so anything to do with turning on a computer and getting it to work makes me feel like a monkey. Eventually, I learned that software is a lot like building Lego sets &amp;ndash; the instructions tell you everything you need to know.</description>
    </item>
    
    <item>
      <title>My memories are stored in melodies</title>
      <link>https://blakeruprecht.github.io/blog/my-memories-in-melodies/</link>
      <pubDate>Thu, 09 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/blog/my-memories-in-melodies/</guid>
      <description>The greatest distance between two places is through time. When I listen to music, occasionally a song will play that instantly transports me back to a specific moment in time, filling me with vivid memories of a moment I&amp;rsquo;ll never forget. When I play through these songs, I relive moments like they just happened.
Somewhere in the Middle One moment, it&amp;rsquo;s a hot and humid day somewhere in the middle of a damp Missouri summer.</description>
    </item>
    
    <item>
      <title>Sitting naked in a snowy field</title>
      <link>https://blakeruprecht.github.io/blog/sitting-naked/</link>
      <pubDate>Thu, 09 Nov 2023 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/blog/sitting-naked/</guid>
      <description>The Basics I want to enjoy life as much as possible. I can guarantee that you do too. Everyone I know pretty much wants the same things out of life &amp;ndash; friendships, good food, plenty of sleep, a healthy body, a calm mind. Tons of money? Only insofar as it buys you food, a place to live, plenty of warm clothes, and some beers to share with our friends. Most of the time, we seek things outside of the basics because of what we lack.</description>
    </item>
    
    <item>
      <title>Something I should say more</title>
      <link>https://blakeruprecht.github.io/blog/i-love-you/</link>
      <pubDate>Fri, 06 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/blog/i-love-you/</guid>
      <description>The tattoo on my achilles heel is an X. If you squint, it kind of looks like an intersection, or a crossroad. To me, this represents indecision, my biggest weakness, and the thing I regret most in life &amp;ndash; not taking action to pursue love over fear. As I sit here at another crossroad, I can&amp;rsquo;t help but think of the lyrics from the song that inspired the ink:
I&amp;rsquo;m standing at the crossroad,</description>
    </item>
    
    <item>
      <title>Packing light for world travel is easy</title>
      <link>https://blakeruprecht.github.io/blog/packing-light/</link>
      <pubDate>Mon, 18 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/blog/packing-light/</guid>
      <description>Since I became an adult, I&amp;rsquo;ve always set out to travel with just one backpack so I never have to wait at that stupid baggage carousel thing. It&amp;rsquo;s never been a problem.
I recently completed a four month trip around Southeast Asia and Europe lasting from February through June. Prior to this, I traveled around the USA pretty extensively for some months. I&amp;rsquo;ve traveled a lot over the past few years, but this was by far the most travel I&amp;rsquo;ve ever done in a one-year period.</description>
    </item>
    
    <item>
      <title>Set grocery shopping PRs</title>
      <link>https://blakeruprecht.github.io/blog/grocery-shopping/</link>
      <pubDate>Fri, 15 Sep 2023 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/blog/grocery-shopping/</guid>
      <description>I went grocery shopping with my buddy recently and was totally baffled when we did the unthinkable: we backtracked. We went back to the produce section not once, but twice, making three total trips through the same section. Grocery shopping is my least favorite part of the eating experience, yet it seems to be a necessity for most people. My garden is non-existent, and my foraging skills are abysmal, so the next best option is to take my hard-earned cash into the big unwindowed box of fluorescent lighting and try to navigate the dense forest of packaged items.</description>
    </item>
    
    <item>
      <title>Growth Mindset</title>
      <link>https://blakeruprecht.github.io/blog/growth-mindset/</link>
      <pubDate>Sun, 06 Aug 2023 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/blog/growth-mindset/</guid>
      <description>SUMMARY: Change your mindset to prioritize growth by praising yourself for effort, hard-work, and learning. Talk to yourself like a friend, don&amp;rsquo;t try to be perfect, and engage in positive self-talk. Eventually, you will be making more progress than you ever were with a fixed mindset.
1. What is growth mindset A growth mindset is when you believe you can improve at anything with time and effort. Instead of seeing challenges as an internal problem, you see them as a chance to get better and improve.</description>
    </item>
    
    <item>
      <title>Easy cold brew method</title>
      <link>https://blakeruprecht.github.io/blog/brew-coffee-cold/</link>
      <pubDate>Thu, 27 Jul 2023 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/blog/brew-coffee-cold/</guid>
      <description>Summary of this essay Why brew yourself? How to brew well Water: 40% of the flavor Fresh beans: 40% of the flavor Freshly ground: 10% of the flavor Brewing consistency: 10% of the flavor The process References Why brew yourself? I go to coffee shops and do not understand how cold brew coffee costs significantly more than hot brew. It must be down to marketing, and the fact that people who don&amp;rsquo;t give a hoot typically aren&amp;rsquo;t going to coffee shops.</description>
    </item>
    
    <item>
      <title>LSTM units incorporate temporal data into NNs</title>
      <link>https://blakeruprecht.github.io/blog/long-short-term-memory/</link>
      <pubDate>Mon, 24 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/blog/long-short-term-memory/</guid>
      <description>Utilizes temporal data, aka LSTM units can use previous information in their determination of current state.
![[transformer diagram.png]]
The first sigmoid is the &amp;ldquo;forget-gate&amp;rdquo; $f_t$, the second sigmoid is the &amp;ldquo;input-gate&amp;rdquo;, $i_t$, the tanh is the &amp;ldquo;cell-update&amp;rdquo;, $\tilde{C}_t$. Forget-gate will determine how much of the previous cell-state we need to remember, input-gate determines how much of the new cell-state we need to remember, and cell-update gate normalizes the cell-state between [-1,1] because of the tanh.</description>
    </item>
    
    <item>
      <title>drawing</title>
      <link>https://blakeruprecht.github.io/drawing/</link>
      <pubDate>Mon, 10 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/drawing/</guid>
      <description>My favorite media are ink drawing and woodworking, here are a few of my best pieces from over the years.
Done using sharpie on paper, based on the houses from Your Cabin in the Woods by Conrad Meinecke.
Done using a dip pen and black india ink. This is Fallingwater, a house designed by Frank Lloyd Wright in 1935.
&amp;hellip;enough, it wouldn&amp;rsquo;t be the minimum. From KISS
I particularly like black ink drawings with that bold, all-black shading style, though it&amp;rsquo;s very hard to get right.</description>
    </item>
    
    <item>
      <title>woodwork</title>
      <link>https://blakeruprecht.github.io/woodwork/</link>
      <pubDate>Mon, 10 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/woodwork/</guid>
      <description>I love building stuff out of wood. I love natural materials, and wood is special because it&amp;rsquo;s perfectly imperfect. Getting a naturally curvy material to make 90 degree corners and straight lines is tough, especially with limited tools (most of this was done with a drill, circ saw, and palm sander). Like with any DIY, I love how I can custom design everything to fit my needs.
Not very complicated, but hey, that&amp;rsquo;s the joy of doing it yourself.</description>
    </item>
    
    <item>
      <title>AIs learn through a variety of learning paradigms.</title>
      <link>https://blakeruprecht.github.io/blog/machine-learning-paradigms/</link>
      <pubDate>Wed, 15 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/blog/machine-learning-paradigms/</guid>
      <description>date +++
Supervised Data comes with labels. The network learns the labels associated with the data. It works pretty well, given the labels actually fit the data, otherwise you get lots of False Positives and False Negatives, semantically.
Unsupervised Different techniques are used to learn structure from data. Problem, is there any true underlying structure to data? Do clusters actually exist? It&amp;rsquo;s useful to cluster often, but hard to prove validity.</description>
    </item>
    
    <item>
      <title>ANFIS has gradient problems</title>
      <link>https://blakeruprecht.github.io/blog/anfis/</link>
      <pubDate>Thu, 15 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/blog/anfis/</guid>
      <description>See my research for more info!
ANFIS has gradient problems during backpropagation. ANFIS is a type of [[neural networks]] trying to learn logical fuzzy rules. It is completely described using the three following equations. These equations are differentiable (mostly), so can be learned using neural techniques. The ws represent antecedents, the zs represent consequents, and the ys represent rule outputs.
$$ w_n^r = \prod_{k=1}^K A_k^r (x_n(k)) $$ $$ z_n^r = \rho_{bias}^r + \sum_{k=1}^K \rho_k^r (x_n(k)) $$ $$ y^r = \frac{\sum_{n=1}^N w_n^r \cdot z_n^r}{\sum_{n=1}^N w_n^r} $$</description>
    </item>
    
    <item>
      <title>Neural gas learns a self-organizing map</title>
      <link>https://blakeruprecht.github.io/blog/neural-gas/</link>
      <pubDate>Sun, 27 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/blog/neural-gas/</guid>
      <description>Similar to [[Kohonen map]], a Self-organing Map, but the topology is learned. Goal is to learn a topological structure to relate &amp;ldquo;close&amp;rdquo; datapoints to each other. Winner-takes-most. Growing neural gas starts with two-neurons and keeps adding a bisecting neuron until the stopping criteria is met (e.g. performance measure below threshold)
Algorithm Given an input dataset $x \in \mathbb{R}^n$ Init &amp;ldquo;weights&amp;rdquo; $w_i \in \mathbb{R}^n$ Pick a datapoint $v$ from $x$, For each weight $w_i$, calculate the distance to $v$ For each weight, determine the number (cardinality?</description>
    </item>
    
    <item>
      <title>The Choquet Integral generalizes many metric functions.</title>
      <link>https://blakeruprecht.github.io/blog/choquet-integral/</link>
      <pubDate>Wed, 04 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/blog/choquet-integral/</guid>
      <description>Where $\pi(j)$ sorts the inputs based on their output values, $h_{\pi(j)}$, which is the output for input $x_j$. Little g is a measure such that $g(A) &amp;lt; g(B)$ if $A \subseteq B$, $g(\emptyset) = 0$, $g(\mathbf{X}) = 1$. ChI adheres to monotonicity, idempotency, subcontinuity, etc. This includes the Linear Order Statistic (LOS) the Order Weighted Average (OWA), Maximum, Minimum, etc. $$ \int h \odot g = C_g(h) = \sum_{j=1}^N h_{\pi(i)} \left( g(A_{\pi(j)}) - g(A_{\pi(j-1)}) \right) $$ $\pi$ is the function that sorts the inputs according to their values $h(x_j)$ $A_{\pi(j)}$ is the sorted vector = ${x_{\pi(1)}, x_{\pi(2)}, &amp;hellip;}$ $g$ is the learned measure (subsumes LOS, OWA, etc.</description>
    </item>
    
    <item>
      <title>Multilayer Perceptrons are the basic unit of a NN</title>
      <link>https://blakeruprecht.github.io/blog/multilayer-perceptrons/</link>
      <pubDate>Sat, 29 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/blog/multilayer-perceptrons/</guid>
      <description>A type of NN that implements multiple layers of the [[Perceptron]].
$\mathbf{x}$ = input vector of length $N$
$\mathbf{w}$ = weight vector of length $N$
$\sigma$ = activation function (typically ReLU, tanh, sigmoid, etc.)
$y$ = output scalar
$\hat{y}$ = label scalar
$e$ = error or loss scalar function (typically 1/2e^2)
$$ y = \sigma(w^T \cdot x) $$ $$ e = \frac{1}{2}(\hat{y} - y)^2$$</description>
    </item>
    
    <item>
      <title>Neural Networks (NNs) approximate any function</title>
      <link>https://blakeruprecht.github.io/blog/neural-networks/</link>
      <pubDate>Tue, 11 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/blog/neural-networks/</guid>
      <description>Neural networks are universal function approximators, according to the Universal function approximation theorem. [[multilayer perceptrons]]. Neural Networks eventually got deeper and more sophisticated, becoming the single most useful tool for artificial intelligence.
Activation Functions ReLU TanH Sigmoid Loss Functions (1/2)e^2 Softmax $$ \sigma(z)i = \frac{\exp{z_i}}{\Sigma{j=1}^K \exp{z_j}} $$ $z$ is the input vector, $\in {-\infty,\infty}$, can be pos, neg, zero. $\exp{z}$ is applied to each element of the input vector, gives a pos.</description>
    </item>
    
    <item>
      <title>Full Flavor, 99 Calories</title>
      <link>https://blakeruprecht.github.io/blog/full-flavor-99-calories/</link>
      <pubDate>Wed, 19 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/blog/full-flavor-99-calories/</guid>
      <description>&amp;ldquo;I&amp;rsquo;ll have to check, we might be out of Bud Select today, what do you want instead —- Bud Light?&amp;rdquo; No, you can&amp;rsquo;t replace the best type of Bud&amp;hellip;
It seems so impossible before it happens. You see it approaching. You see all of the warning signs, all of the speed bumps preparing you for the abrupt stop ahead. Somber stories are repeated from old friends who are long past the days of sitting on rooftops at 2pm on a Tuesday, skipping class and drinking cold ones out of a squat, plastic cup, deciding that their responsibilities can be pushed off for at least two more hours, until they hit that Ballmer peak of &amp;ldquo;creativity&amp;rdquo; or whatever you&amp;rsquo;re supposed to call drunkenly typing meandering lines of thought and utilizing spell check way too much and re-doing it all the next morning anyway &amp;ndash; but that doesn&amp;rsquo;t matter.</description>
    </item>
    
    <item>
      <title>research</title>
      <link>https://blakeruprecht.github.io/research/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://blakeruprecht.github.io/research/</guid>
      <description>Previously, I was an Artificial Intelligence research assistant at the University of Missouri (MIZ-ZOU!) working with Derek Anderson and the Center for Geospatial Intelligence. Our work focused on eXplainable AI (XAI), specifically developing new neuro-fuzzy systems that explain their own decision-making steps.
What did I research? I went to grad school to research the AI Alignment problem, which is similar to AI safety, AI risk research, etc. Alignment is more specific than safety because it doesn&amp;rsquo;t just mean trustworthy and harmless, it also means beneficial.</description>
    </item>
    
  </channel>
</rss>
