---
title: How to Get Better Responses from ChatGPT
date: 2024-07-16
description: '"Generative AI Large Language Models are great at filling in the gaps from input to output. The best way to use a LLM is to describe the output you want as clearly as possible."'
---
Modern generative AIs are getting very, very good at responding to questions. Generative AI Large Language Models (LLMs) like ChatGPT, Claude, Gemini, and LLama all respond to language prompts excellently. We are collectively starting to realize the benefits of using AI for work and play. But how do you get the most out of your interactions with these new weird machines?

Suppose you want to learn how to code. Five years ago, this was very difficult, even with Google. Now with Generative AIs, you have your own personal coding tutor at your fingertips, ready to give you live tutorials on how to do anything, with almost infinitely more information ready to be accessed. But with so much information, how do you get the LLM to give you the information you want?

Work backwards.

As Stephen Covey says in 7 Habits, "begin with the end in mind." Generative AIs are great at filling in the details so you don't have to. Suppose you want to learn how to create your own Large Language Model. You have no idea how to code. Easy, just tell the LLM where you want to end up, and tell it how you want to get there. It'll do the rest.

Here's a prompt I gave ChatGPT:

> Show me how to make a LLM of my own. I don't know how to code. I want to end up with all of the necessary code on my computer to run it myself. Give me the list of steps I need. For each step, include verification questions to make sure I've successfully completed that step. Include relevant links to outside tutorials if I want to learn more.

This short prompt outputs almost 1000 words generated by ChatGPT to take me step by step through installing Python, installing other software, loading my own data, writing code, and running code. The output you get may be slightly different, but that's okay — you can mold the output to be whatever you want.

Depending on your programming experience, any one of these steps may look totally foreign to you. You can seamlessly ask for more or less information about any step, while maintaining the rest of the output.

> I don't know what a terminal is. Include a step in the original instructions that helps me figure out what this is and how to use it.

See the trick? "Include a step…" prompts ChatGPT to keep the original steps and add in a new one. All I had to do was tell it what I wanted. You can do this with any prompt. Result not good enough? Add more detail about what response you're expecting, and the AI will generate more information.

The trick to using LLMs efficiently is to specify the exact output you want in as much detail as possible. If you know where you want to end up, it's just a matter of asking the right questions, and leaving the rest up to the machines to figure out.
